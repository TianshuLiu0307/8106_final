---
title: "Final Project Code"
author: "Tianshu Liu, Lincole Jiang, Jiong Ma"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
    number_sections: true
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
editor_options: 
  chunk_output_type: console
---
\newpage



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE)
```

```{r library}
library(tidyverse)
library(summarytools)
library(corrplot)
library(caret)
library(vip)
library(rpart.plot)
library(ranger)
```

# Data Import
```{r import_data}
# import data
load("./recovery.RData")

set.seed(3196) 
lts.dat <- dat[sample(1:10000, 2000),]
set.seed(2575)
lincole.dat <- dat[sample(1:10000, 2000),]
set.seed(5509)
amy.dat <- dat[sample(1:10000, 2000),]

dat1 <- lts.dat %>% 
  merge(lincole.dat, all = TRUE) %>% 
  na.omit() %>% 
  select(-id) %>% 
  mutate(
    gender = as.factor(gender),
    race = as.factor(race),
    smoking = as.factor(smoking),
    hypertension = as.factor(hypertension),
    diabetes = as.factor(diabetes),
    vaccine = as.factor(vaccine),
    severity = as.factor(severity),
    study = as.factor(study))
  
dat2 <- lts.dat %>% 
  merge(amy.dat, all = TRUE) %>% 
  na.omit() %>% 
  select(-id) %>% 
  mutate(
    gender = as.factor(gender),
    race = as.factor(race),
    smoking = as.factor(smoking),
    hypertension = as.factor(hypertension),
    diabetes = as.factor(diabetes),
    vaccine = as.factor(vaccine),
    severity = as.factor(severity),
    study = as.factor(study))

dat3 <- lincole.dat %>% 
  merge(amy.dat, all = TRUE) %>% 
  na.omit() %>% 
  select(-id) %>% 
  mutate(
    gender = as.factor(gender),
    race = as.factor(race),
    smoking = as.factor(smoking),
    hypertension = as.factor(hypertension),
    diabetes = as.factor(diabetes),
    vaccine = as.factor(vaccine),
    severity = as.factor(severity),
    study = as.factor(study))

dat <- dat1
summary(dat)

bin.dat <- dat %>% 
  mutate(recovery_time = ifelse(recovery_time > 30, "gt30", "lt30")) %>% 
  mutate(recovery_time = factor(recovery_time, levels = c("lt30", "gt30")))

summary(bin.dat)
```

# Data partition

```{r data_partition}
# data partition
dat.matrix <- model.matrix(recovery_time ~ ., dat)[ ,-1]

set.seed(1)
trainRows <- createDataPartition(y = dat$recovery_time, p = 0.8, list = FALSE)

train.dat <- dat[trainRows,]
train.bin.dat <- bin.dat[trainRows,]

train.dat.matrix <- model.matrix(~., train.dat)[, -1]
train.bin.dat.matrix <- train.dat.matrix %>% 
  as.data.frame() %>% 
 mutate(recovery_time = ifelse(recovery_time > 30, "gt30", "lt30")) %>% 
  mutate(recovery_time = factor(recovery_time, levels = c("lt30", "gt30"))) 

train.x <- dat.matrix[trainRows,]
train.y <- dat$recovery_time[trainRows]
train.bin.y <- bin.dat$recovery_time[trainRows]

test.x <- dat.matrix[-trainRows,]
test.y <- dat$recovery_time[-trainRows]
test.bin.y <- bin.dat$recovery_time[-trainRows]
```

# Primary Analysis
## Exploratory analysis and data visualization

```{r primary_summary, results = 'asis'}
# data summary
st_options(plain.ascii = FALSE,
           style = "rmarkdown",
           dfSummary.silent = TRUE,
           footnote = NA,
           subtitle.emphasis = FALSE)
dfSummary(train.dat)

skimr::skim_without_charts(train.dat)
```


```{r pri_eda, fig.show='true'}
####################################################################
## Remember to edit the next chunk if you do any modification here:)
####################################################################

# EDA
cts_var = c("age", "height", "weight", "bmi", "SBP", "LDL")
fct_var = c("gender", "race", "smoking", "hypertension", "diabetes", "vaccine", "severity", "study")

# scatter plot of continuous predictors
par(mfrow=c(2, 3))
for (i in 1:length(cts_var)){
  var = cts_var[i]
  plot(recovery_time~train.dat[,var],
       data = train.dat,
       ylab = "recovery time",
       xlab = var,
       main = str_c("Scatter Plot of ", var))
  lines(stats::lowess(train.dat[,var], train.dat$recovery_time), col = "red", type = "l")
}
for (i in 1:length(cts_var)){
  var = cts_var[i]
  hist(train.dat[,var], 
       ylab = "recovery_time", 
       xlab = var, 
       main = str_c("Histogram of ", var)) 
}

# boxplot of categorical predictors
par(mfrow=c(2, 4))
for (i in 1:length(fct_var)){
  var = fct_var[i]
  plot(recovery_time~train.dat[,var],
       data = train.dat,
       ylab = "recovery_time", 
       xlab = var, 
       main = str_c("Boxplot of ", var))
}

# histogram of response
par(mfrow=c(1, 1))
hist(train.dat$recovery_time, 
     breaks = 50, 
     main = "Histogram of recovery_time", 
     xlab = "recovery_time")

# correlation
par(mfrow=c(1, 1))
corrplot(cor(train.dat[,cts_var]), method = "circle", type = "full", 
         title = "Correlation plot of continuous variables", 
         mar = c(2, 2, 4, 2))
```

```{r pri_eda_save_plot, include=FALSE}
# this chunk is used just for saving codes
# create folder for figures
folder_path <- "./figure/"
if (!file.exists(folder_path)) {
  dir.create(folder_path, recursive = TRUE)
} else{print("...")}

# EDA
# scatter plot of continuous predictors
jpeg("./figure/eda1_sactter.jpeg", width=8, height=6, units="in", res=500)
par(mfrow=c(2, 3))
for (i in 1:length(cts_var)){
  var = cts_var[i]
  plot(recovery_time~train.dat[,var],
       data = train.dat,
       ylab = "recovery_time",
       xlab = var,
       main = str_c("Scatter plot of ", var))
  lines(stats::lowess(train.dat[,var], train.dat$recovery_time), col = "red", type = "l")
}
dev.off()

# histograms of predictors
jpeg("./figure/eda1_hist.jpeg", width=8, height=6, units="in", res=500)
par(mfrow=c(2, 3))
for (i in 1:length(cts_var)){
  var = cts_var[i]
  hist(train.dat[,var], 
       ylab = "recovery_time", 
       xlab = var, 
       main = str_c("Histogram of ", var)) 
}
dev.off()


# boxplot of categorical predictors
jpeg("./figure/eda1_boxplot.jpeg", width = 10, height=6, units="in", res=500)
par(mfrow=c(2, 4))
for (i in 1:length(fct_var)){
  var = fct_var[i]
  plot(recovery_time~train.dat[,var],
       data = train.dat,
       ylab = "recovery_time", 
       xlab = var, 
       main = str_c("Boxplot of ", var))
}
dev.off()

# histogram of response
jpeg("./figure/eda1_res_hist.jpeg", width = 8, height=6, units="in", res=500)
par(mfrow=c(1, 1))
hist(train.dat$recovery_time, 
     breaks = 50, 
     main = "Histogram of recovery_time", 
     xlab = "recovery_time")
dev.off()

# correlation
jpeg("./figure/eda1_corr.jpeg", width = 5, height=5, units="in", res=500)
par(mfrow=c(1, 1))
corrplot(cor(train.dat[,cts_var]), method = "circle", type = "full", 
         title = "Correlation plot of continuous variables", 
         mar = c(2, 2, 4, 2))
dev.off()
```

## Model Training

```{r ctrl1}
ctrl1 <- trainControl(method = "cv", number = 5)
```

### Linear Model

```{r linear}
set.seed(1)

lm.fit <- train(train.x, train.y,
               method = "lm",
               trControl = ctrl1)

coef(lm.fit$finalModel)

vip(lm.fit$finalModel)
```

### LASSO

```{r lasso}
set.seed(1)
lasso.fit <- train(train.x, train.y,
                   method = "glmnet",
                   tuneGrid = expand.grid(
                     alpha = 1,
                     lambda = exp(seq(0, -7, length=100))),
                   trControl = ctrl1)

lasso.fit$bestTune

coef(lasso.fit$finalModel, s = lasso.fit$bestTune$lambda)

ggplot(lasso.fit, highlight = TRUE) + 
  labs(title="LASSO CV Result") +
  scale_x_continuous(trans='log',n.breaks = 10) +
  theme_bw()
ggsave("./figure/lasso_cv.jpeg", dpi = 500)

vip(lasso.fit$finalModel)
```


### Ridge

```{r ridge}
set.seed(1)
ridge.fit <- train(train.x, train.y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0,
                                          lambda = exp(seq(1, -5, length=100))), 
                   trControl = ctrl1)

ridge.fit$bestTune

coef(ridge.fit$finalModel, s = ridge.fit$bestTune$lambda)

ggplot(ridge.fit,highlight = TRUE) + 
  scale_x_continuous(trans='log', n.breaks = 6) +
  labs(title="Ridge CV Result") +
  theme_bw()
ggsave("./figure/ridge_cv.jpeg", dpi = 500)

vip(ridge.fit$finalModel)
```


### Elastic Net

```{r enet}
set.seed(1)
enet.fit <- train(train.x, train.y,
                  method = "glmnet",
                  tuneGrid = expand.grid(
                    alpha = seq(0, 1, length = 21),
                    lambda = exp(seq(0, -8, length = 50))),
                  trControl = ctrl1)

enet.fit$bestTune

coef(enet.fit$finalModel, enet.fit$bestTune$lambda)


ggplot(enet.fit, highlight = TRUE) + 
  scale_x_continuous(trans='log', n.breaks = 6) +
  labs(title ="Elastic Net CV Result") + 
  theme_bw()

ggsave("./figure/enet_cv.jpeg", dpi = 500)

vip(enet.fit$finalModel)
```

### Principal components regression (PCR)

```{r pcr}
set.seed(1)
pcr.fit <- train(train.x, 
                 train.y,
                 method = "pcr",
                 tuneGrid  = data.frame(ncomp = 1:ncol(train.x)),
                 trControl = ctrl1,
                 preProcess = c("center", "scale"))
ggplot(pcr.fit, highlight = TRUE) + 
  labs(title  ="PCR CV Result") +
  theme_bw()

ggsave("./figure/pcr_cv.jpeg", dpi = 500)

pcr.fit$bestTune
coef(pcr.fit$finalModel)

vip(pcr.fit$finalModel)
```

### Partial Least Squares (PLS)

```{r pls}
set.seed(1)
pls.fit <- train(train.x, 
                 train.y,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:ncol(train.x)),
                 trControl = ctrl1,
                 preProcess = c("center", "scale"))
ggplot(pls.fit, highlight = TRUE) + 
  labs(title  ="PLS CV Result") +
  theme_bw()

ggsave("./figure/pls_cv.jpeg", dpi = 500)

pls.fit$bestTune
coef(pls.fit$finalModel)

vip(pls.fit$finalModel)
```

### Generalized Additive Model (GAM)

```{r gam}
set.seed(1)
gam.fit <- train(train.x, 
                 train.y,
                 method = "gam",
                 tuneGrid = data.frame(select = c(TRUE, FALSE), 
                                       method = "GCV.Cp"),
                 trControl = ctrl1)


ggplot(gam.fit) +
  labs(title = "GAM CV Result") + 
  theme_bw()
ggsave("./figure/gam_cv.jpeg", dpi = 500)

gam.fit$bestTune

# coef(gam.fit$finalModel)
gam.fit$finalModel

par(mfrow=c(2, 3))
plot(gam.fit$finalModel)
par(mfrow=c(1, 1))
```

### Multivariate Adaptive Regression Splines (MARS)

```{r mars}
mars.grid <- expand.grid(degree = 1:4,
                         nprune = 2:14)
set.seed(1)
mars.fit <- train(train.x, 
                  train.y,
                  method = "earth",
                  tuneGrid = mars.grid,
                  trControl = ctrl1)

ggplot(mars.fit, highlight = TRUE)+ 
  labs(title  ="MARS CV Result") +
  theme_bw()

ggsave("./figure/mars_cv.jpeg", dpi = 500)

mars.fit$bestTune

coef(mars.fit$finalModel)

summary(mars.fit$finalModel)

vip(mars.fit$finalModel)
```

### K-Nearest Neighbour (KNN)

```{r knn}
set.seed(1)
knn.fit <- train(train.x, 
                train.y,
                tuneGrid  = data.frame(k = 1:20),
                method = "knn",
                trControl = ctrl1)

ggplot(knn.fit, highlight = TRUE) + 
  labs(title  ="KNN CV Result") +
  theme_bw()

ggsave("./figure/knn_cv.jpeg", dpi = 500)

knn.fit$bestTune
```

### Bagging

```{r bagging, cache=TRUE}
bag.grid <- expand.grid(mtry = ncol(train.x),
                       splitrule = "variance",
                       min.node.size = 1:20)
set.seed(1)
bag.fit <- train(train.x, 
                 train.y,
                 method = "ranger",
                 tuneGrid = bag.grid, 
                 trControl = ctrl1)

bag.fit$bestTune

ggplot(bag.fit, highlight = TRUE) + 
  labs(title = "Bagging CV Result") + 
  theme_bw()
ggsave("./figure/bagging_cv.jpeg", dpi = 500)

bag.final.per <- ranger(recovery_time ~ .,
                        data = train.dat.matrix, 
                        mtry = ncol(train.x),
                       splitrule = "variance",
                       min.node.size = bag.fit$bestTune[[3]],
                       importance = "permutation",
                       scale.permutation.importance = TRUE)

barplot(sort(ranger::importance(bag.final.per), 
             decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(ncol(train.x)))

# p1 <- pdp::partial(
#   bag.fit, 
#   pred.var = "Lot_Area",
#   grid.resolution = 20
#   ) %>% 
#   autoplot()
# p2 <- pdp::partial(
#   bag.fit, 
#   pred.var = "Lot_Frontage", 
#   grid.resolution = 20
#   ) %>% 
#   autoplot()
# gridExtra::grid.arrange(p1, p2, nrow = 1)
```


### Random Forest

```{r rf, cache=TRUE}
rf.grid <- expand.grid(mtry = 1:ncol(train.x),
                       splitrule = "variance",
                       min.node.size = seq(8, 18, by = 2))
set.seed(1)
rf.fit <- train(train.x, 
                train.y,
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl1)

rf.fit$bestTune

ggplot(rf.fit, highlight = TRUE) + 
  labs(title = "Random Forest CV Result") + 
  theme_bw()

ggsave("./figure/rf_cv.jpeg", dpi = 500)

rf.final.per <- ranger(recovery_time ~ .,
                       data = train.dat.matrix, 
                       mtry = rf.fit$bestTune[[1]],
                       splitrule = "variance",
                       min.node.size = rf.fit$bestTune[[3]],
                       importance = "permutation",
                       scale.permutation.importance = TRUE)

barplot(sort(ranger::importance(rf.final.per), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(ncol(train.x)))
```


### Boosting

```{r boosting, cache=TRUE}
set.seed(1)
bst.grid <- expand.grid(n.trees = c(7000, 8000, 9000),
                        interaction.depth = 1:4,
                        shrinkage = c(0.0005, 0.001,0.002),
                        n.minobsinnode = c(1,10))

bst.fit <- train(train.x, 
                 train.y,
                 method = "gbm",
                 tuneGrid = bst.grid,
                 trControl = ctrl1,
                 verbose = FALSE)

bst.fit$bestTune

ggplot(bst.fit, highlight = TRUE) + 
  labs(title = "Boosting CV Result") + 
  theme_bw()

ggsave("./figure/boosting_cv.jpeg", dpi = 500)

# Variable Importance
summary(bst.fit$finalModel, las = 2, cBars = ncol(train.x), cex.names = 0.6)
```


### Regression Trees

```{r rpart}
rpart.grid <- expand.grid(cp = exp(seq(-5,-2, length = 50)))
set.seed(1)
rpart.fit1 <- train(train.x, 
                   train.y,
                   method = "rpart",
                   tuneGrid = rpart.grid,
                   trControl = ctrl1)

ggplot(rpart.fit1, highlight = TRUE) +
  labs(titlem = "Regression Tree CV Result") +
  theme_bw()

ggsave("./figure/rpart1_cv.jpeg", dpi = 500)

rpart.fit1$bestTune

rpart.plot(rpart.fit1$finalModel)

jpeg("./figure/rpart1.jpeg", width = 8, height = 6, units="in", res=500)
rpart.plot(rpart.fit1$finalModel)
dev.off()
```


## Model Selection

```{r resample1}
set.seed(1)
resamp <- resamples(list(lm = lm.fit,
                         lasso = lasso.fit,
                         ridge = ridge.fit,
                         enet = enet.fit,
                         pcr = pcr.fit,
                         pls = pls.fit,
                         gam = gam.fit,
                         mars = mars.fit,
                         knn = knn.fit, 
                         bagging = bag.fit, 
                         rf = rf.fit,
                         boosting = bst.fit,
                         tree = rpart.fit1))

summary(resamp)

p1=bwplot(resamp, metric = "RMSE")
p2=bwplot(resamp, metric = "Rsquared")
grid.arrange(p1, p2 ,ncol=2)

jpeg("./figure/resample1.jpeg", width = 8, height=6, units="in", res=500)
p1=bwplot(resamp, metric = "RMSE")
p2=bwplot(resamp, metric = "Rsquared")
grid.arrange(p1, p2, ncol=2)
dev.off()
```

```{r interpret}
p1<- pdp::partial(mars.fit, pred.var = c("bmi"), grid.resolution = 10) %>% autoplot() + 
  theme_bw()+ 
  labs(title = "Partial Dependence Plots of MARS Model")

p2 <-pdp::partial(mars.fit, pred.var = c("bmi", "studyB"),
                   grid.resolution = 10) %>%
      pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE,
                       screen = list(z = 20, x = -60))

# jpeg("./figure/partial_dependence.jpeg", width = 8, height=6, units="in", res=500)
gridExtra::grid.arrange(p1, p2, ncol = 2)
# dev.off()

# Important variables
varImp(mars.fit$finalModel)
```

## Training / Testing Error

```{r err}
# training error
mars.train.pred = predict(mars.fit, newdata = train.x)
RMSE(train.y, mars.train.pred)

# testing error
mars.pred = predict(mars.fit, newdata = test.x)
RMSE(test.y, mars.pred)
```

# Secondary Analysis

## Exploratory analysis and data visualization

```{r sec_summary, results = 'asis'}
# data summary
st_options(plain.ascii = FALSE,
           style = "rmarkdown",
           dfSummary.silent = TRUE,
           footnote = NA,
           subtitle.emphasis = FALSE)
dfSummary(train.bin.dat)

skimr::skim_without_charts(train.bin.dat)
```


```{r sec_eda, fig.show='true'}
####################################################################
## Remember to edit the next chunk if you do any modification here:)
####################################################################
# EDA

# boxplot of continuous predictors
par(mfrow=c(2, 3))
for (i in 1:length(cts_var)){
  var = cts_var[i]
  boxplot(train.bin.dat[,var]~recovery_time,
       data = train.bin.dat,
       xlab = "recovery time",
       ylab = var, 
       main = str_c("Boxplot of ", var))
}

# barplot of categorical predictors
par(mfrow=c(2, 4))
for (i in 1:length(fct_var)){
  var <- fct_var[i]
  counts <- table(train.bin.dat[,var], train.bin.y)
  barplot(counts, beside = TRUE, legend.text = TRUE,
        xlab = "recovery time", 
        ylab = "Count", 
        main = str_c("Barplot of ", var), 
        args.legend = list(bty = 'n', x = 'topleft'))
}

# barplot of response
par(mfrow=c(1, 1))
counts <- table(train.bin.y)
barplot(counts, 
        xlab = "recovery time", 
        ylab = "Count", 
        main = "Barplot of binary recovery_time")
```

```{r sec_eda_save_plot, include=FALSE}
# this chunk is used just for saving codes
# EDA

# boxplot of continuous predictors
jpeg("./figure/eda2_boxplot.jpeg", width=8, height=6, units="in", res=500)
par(mfrow=c(2, 3))
for (i in 1:length(cts_var)){
  var = cts_var[i]
  boxplot(train.bin.dat[,var]~recovery_time,
       data = train.bin.dat,
       xlab = "recovery time",
       ylab = var,
       main = str_c("Boxplot of ", var))
}
dev.off()

# barplot of categorical predictors
jpeg("./figure/eda2_barplot.jpeg", width=8, height=6, units="in", res=500)
par(mfrow=c(2, 4))
for (i in 1:length(fct_var)){
  var <- fct_var[i]
  counts <- table(train.bin.dat[,var], train.bin.y)
  barplot(counts, beside = TRUE, legend.text = TRUE,
        xlab = "recovery time", 
        ylab = "Count", 
        main = str_c("Barplot of ", var), 
        args.legend = list(bty = 'n', x = 'topleft'))
}
dev.off()

# barplot of response
jpeg("./figure/eda2_res_barplot.jpeg", width = 8, height=6, units="in", res=500)
par(mfrow=c(1, 1))
counts <- table(train.bin.y)
barplot(counts, 
        xlab = "recovery time", 
        ylab = "Count", 
        main = "Barplot of binary recovery_time")
dev.off()
```


## Model Training

### Logistic Regression

```{r logistic}
set.seed(1)
glm.fit <- train(x = train.x, 
                 y = train.bin.y,
                 method = 'glm',
                 trControl = ctrl1)
coef(glm.fit$finalModel)
vip(glm.fit$finalModel) + theme_bw()
```


### Penalized Logistic Regression

```{r plr}
glmnGrid <- expand.grid(.alpha = seq(0, 1, length = 21),
                        .lambda = exp(seq(-10, -5, length = 15)))
set.seed(1)
glmn.fit <- train(train.x, 
                  train.bin.y,
                  method = 'glmnet',
                  tuneGrid = glmnGrid,
                  trControl = ctrl1)

glmn.fit$bestTune

myCol<- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol), 
              superpose.line = list(col = myCol))

ggplot(glmn.fit, highlight = TRUE) + 
  labs(title="Penalized Logistic Regression CV Result") +
  theme_bw()

ggsave("./figure/penal_logi_cv.jpeg", dpi = 500)

#coef(glmn.fit$finalModel)
vip(glmn.fit$finalModel) + theme_bw()
```


### Generalized Additive Model (GAM) for classification

```{r binarygam}
set.seed(1)
gam.bin.fit <- train(train.x, 
                     train.bin.y,
                     method = "gam",
                     trControl = ctrl1)

ggplot(gam.bin.fit) +
  labs(title = "GAM Classification CV Result") + 
  theme_bw()
ggsave("./figure/gam_binned_cv.jpeg", dpi = 500)

gam.bin.fit$bestTune
# coef(gam.fit$finalModel)

par(mfrow=c(2, 3))
plot(gam.bin.fit$finalModel)
par(mfrow=c(1, 1))
```


### Multivariate Adaptive Regression Splines (MARS) for classification

```{r binaryMars}
set.seed(1)
mars.bin.fit <- train(train.x, 
                      train.bin.y,
                      method = "earth",
                      tuneGrid = expand.grid(degree = 1:3,
                                             nprune = 2:ncol(train.x)),
                      trControl = ctrl1)

mars.bin.fit$bestTune

ggplot(mars.bin.fit, highlight = TRUE) + 
  labs(title  ="MARS Classification CV Result") +
  theme_bw()

ggsave("./figure/mars_binned_cv.jpeg", dpi = 500)

mars.bin.fit$bestTune

coef(mars.bin.fit$finalModel) %>% 
  broom::tidy() %>% 
  knitr::kable()
summary(mars.bin.fit$finalModel)
vip(mars.bin.fit$finalModel) + theme_bw()
```

### Linear Discriminant Analysis (LDA)

```{r lda}
set.seed(1)
lda.fit <- train(train.x,
                 train.bin.y,
                 method = "lda",
                 trControl = ctrl1)
```

### Quadratic Discriminant Analysis (QDA)

```{r qda}
set.seed(1)
qda.fit <- train(train.x,
                   train.bin.y,
                   method = "qda",
                   trControl = ctrl1)
```

### Naive Bayes (NB)

```{r nb}
nbGrid <- expand.grid(usekernel = c(FALSE,TRUE),
                      fL = 1,
                      adjust = seq(0.1, 1.2, by = .1))
set.seed(1)
nb.fit <- train(train.x,
                  train.bin.y,
                  method = "nb",
                  tuneGrid = nbGrid,
                  trControl = ctrl1)
nb.fit$bestTune
ggplot(nb.fit, highlight = TRUE) + 
  labs(title  ="Naive Bayes Classification CV Result") +
  theme_bw()

ggsave("./figure/nb_cv.jpeg", dpi = 500)
```


### Bagging

```{r binaryBag}
bag.grid2 <- expand.grid(mtry = ncol(train.x),
                       splitrule = "gini",
                       min.node.size = 1:20)
set.seed(1)
bag.fit2 <- train(train.x, 
                 train.bin.y,
                 method = "ranger",
                 tuneGrid = bag.grid2, 
                 trControl = ctrl1)

bag.fit2$bestTune

ggplot(bag.fit2, highlight = TRUE) + 
  labs(title = "Bagging Classification CV Result") + 
  theme_bw()

ggsave("./figure/bagging_classification_cv.jpeg", dpi = 500)

bag.final.per2 <- ranger(recovery_time ~ .,
                        data = train.bin.dat.matrix, 
                        mtry = ncol(train.x),
                       splitrule = "gini",
                       min.node.size = bag.fit2$bestTune[[3]],
                       importance = "permutation",
                       scale.permutation.importance = TRUE)

barplot(sort(ranger::importance(bag.final.per2), 
             decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(ncol(train.x)))
```


### Random Forest

```{r binaryRF}
rf.grid2 <- expand.grid(mtry = 1:ncol(train.x),
                       splitrule = "gini",
                       min.node.size = seq(10, 20, by=2))
set.seed(1)
rf.fit2 <- train(train.x, 
                train.bin.y,
                method = "ranger",
                tuneGrid = rf.grid2,
                trControl = ctrl1)

rf.fit2$bestTune

ggplot(rf.fit2, highlight = TRUE) + 
  labs(title = "Random Forest Classification CV Result") + 
  theme_bw()
ggsave("./figure/rf_classification_cv.jpeg", dpi = 500)

rf.final.per2 <- ranger(recovery_time ~ .,
                       data = train.bin.dat.matrix, 
                       mtry = rf.fit2$bestTune[[1]],
                       splitrule = "gini",
                       min.node.size = rf.fit2$bestTune[[3]],
                       importance = "permutation",
                       scale.permutation.importance = TRUE)

barplot(sort(ranger::importance(rf.final.per2), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(ncol(train.x)))
```


### Boosting

```{r binaryBoost}
set.seed(1)
bst.grid2 <- expand.grid(n.trees = c(4000, 5000, 6000,7000),
                        interaction.depth = 1:4,
                        shrinkage = c(0.0005, 0.001, 0.002, 0.004),
                        n.minobsinnode = c(1,10))

bst.fit2 <- train(train.x, 
                 train.bin.y,
                 method = "gbm",
                 tuneGrid = bst.grid2,
                 trControl = ctrl1,
                 verbose = FALSE)

bst.fit2$bestTune

ggplot(bst.fit2, highlight = TRUE) + 
  labs(title = "Boosting Classification CV Result") + 
  theme_bw()

ggsave("./figure/boosting_classification_cv.jpeg", dpi = 500)

# Variable Importance
summary(bst.fit2$finalModel, las = 2, cBars = ncol(train.x), cex.names = 0.6)
```


### Classification Trees

```{r binarytree}
rpart.grid = expand.grid(cp = exp(seq(-6,-4, len = 50)))
set.seed(1)
rpart.fit2 <- train(train.x, 
                   train.bin.y, 
                   method = "rpart",
                   tuneGrid = rpart.grid,
                   trControl = ctrl1)

rpart.fit2$bestTune

ggplot(rpart.fit2, highlight = TRUE) +
  labs(title = "Classification Tree CV Result") + 
  theme_bw()

ggsave("./figure/rpart2_cv.jpeg", dpi = 500)

rpart.plot(rpart.fit2$finalModel)

jpeg("./figure/rpart2.jpeg", width = 8, height = 6, units="in", res=500)
rpart.plot(rpart.fit2$finalModel)
dev.off()
```


### Support Vector Machine (SVM)

```{r svmr, cache=TRUE}
svmr.grid <- expand.grid(C = exp(seq(-3, 6, len = 20)),
                         sigma = exp(seq(-4, 2, len = 10)))

set.seed(1)
svmr.fit <- train(train.x, 
                  train.bin.y,
                  method = "svmRadialSigma",
                  tuneGrid = svmr.grid,
                  trControl = ctrl1)

svmr.fit$bestTune
myCol<- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))
ggplot(svmr.fit, highlight = TRUE, par.settings = myPar) + 
  labs(title = "SVM Radial Kernal CV result") + 
  theme_bw()

ggsave("./figure/svmr_cv.jpeg", dpi = 500)

confusionMatrix(svmr.fit)
```


## Model Selection

```{r resample2}
set.seed(1)
resamp <- resamples(list(glm = glm.fit, 
                         glmnet = glmn.fit,
                         gam = gam.bin.fit, 
                         mars = mars.bin.fit, 
                         lda = lda.fit, 
                         qda = qda.fit, 
                         nb = nb.fit,
                         bagging = bag.fit2, 
                         rf = rf.fit2,
                         boosting = bst.fit2, 
                         tree = rpart.fit2, 
                         svm = svmr.fit))

summary(resamp)

p1=bwplot(resamp, metric = "Accuracy")
p2=bwplot(resamp, metric = "Kappa")
grid.arrange(p1, p2 ,ncol=2)

jpeg("./figure/resample2.jpeg", width = 8, height=6, units="in", res=500)
p1=bwplot(resamp, metric = "Accuracy")
p2=bwplot(resamp, metric = "Kappa")
grid.arrange(p1, p2, ncol=2)
dev.off()
```


## Training  / Testing Error
